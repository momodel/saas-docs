# 边学边做

> * 在 Mo 运行你的第一段代码
> * 上传发布一个数据集
> * 在 Notebook 中使用数据集
> * 如何导入并使用模块和数据集
> * 开发和部署一个项目
> * 在 GPU/CPU 资源上训练机器学习模型
> * 利用 TensorBoard 可视化评估模型
> * 把模型转换为 TensorFlow Lite 格式
> * 如何在 Notebook 中节省运行内存

## 在 Mo 运行你的第一段代码

### 1. 在训练营中点击创建项目

你可以在训练营中的[平台功能教程](http://www.momodel.cn:8899/classroom/class?id=5c5696cd1afd9458d456bf54&type=doc)，选择`在Mo运行你的第一段代码`，来按照教程中的`.ipynb`文件指引边学边做，熟悉Mo使用方法和开发流程。

<img src='http://imgbed.momodel.cn/5cc1a291e3067ceb154f0e5c.jpg' width=100% height=100%>

### 2. 在Mo运行你的第一行代码

你可以通过新建一个Notebook文件运行以下代码，也可以直接打开工程项目中的提供的Notebook文件直接运行。

```python
print('Hello, MO!')
```

你将看到

```python
Hello, MO!
```

### 3. 在Mo上你可以使用 Tensorflow

 使用 Mo 可直接在浏览器中执行 TensorFlow 代码。下面的代码示例展示了两个矩阵相加的情况。
<img src='http://imgbed.momodel.cn/5cc1a292e3067ceb154f0e5d.jpg' width=50% height=50%>

```python
import tensorflow as tf
import numpy as np

with tf.Session():
  input1 = tf.constant(np.reshape(np.arange(1.0, 7.0, dtype=np.float32), (2, 3)))
  input2 = tf.constant(1.0, shape=[2, 3])
  output = tf.add(input1, input2)
  result = output.eval()
result
```

你将看到

```python
array([[2., 3., 4.],
       [5., 6., 7.]], dtype=float32)
```

### 4. 在 Mo 上你可以方便地进行数据可视化

你可以使用 Python matplotlib 模块很方便的进行数据可视化。

```python
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
x = np.arange(15)
y = [x_i + np.random.randn() for x_i in x]
a, b = np.polyfit(x, y, 1)
plt.plot(x, y, 'o', np.arange(15), a*np.arange(15)+b, '-');
```

<img src='http://imgbed.momodel.cn/5cc1a291e3067ceb154f0e5a.jpg' width=50% height=50%>

### 5. 在 Mo 上你可以方便使用 SKlearn 等机器学习工具包

Mo 已安装了机器学习中经常会用到的库，例如 tensorflow，sklearn，pytorch等。
下面的代码展示了在Iris数据集上使用sklearn提供的logistic-regression classifier进行分类。

```python
print(__doc__)


# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause

import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model, datasets
%matplotlib inline

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features.
Y = iris.target

h = .02  # step size in the mesh

logreg = linear_model.LogisticRegression(C=1e5)

# we create an instance of Neighbours Classifier and fit the data.
logreg.fit(X, Y)

# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(4, 3))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())

plt.show()
```

<img src='http://imgbed.momodel.cn/5cc1a291e3067ceb154f0e5b.jpg' width=50% height=50%>

## 上传发布一个数据集

### 1. 新建数据集项目

数据集是由数据组成的集合，是模块和项目开发的基础材料。该部分将指引你从创建数据集开始，学习上传和发布一个新的数据集。你可以在训练营中的[平台功能教程](http://www.momodel.cn:8899/classroom/class?id=5c5696cd1afd9458d456bf54&type=doc)，选择`上传发布一个数据集`，来进行实践。

在工作区新建数据集，创建完成后，将进入到上传编辑页面，在这里，你可以上传、删除和解压缩文件，具体参见下面这个视频。

<div key='1'>
<video src="http://files.momodel.cn/dataset.mp4" controls="controls" width="100%" height="100%" />
</div>

### 2. 上传和下载数据

进入刚刚创建的数据集中，点击上传按钮，选择需要上传的数据。平台目前支持 jpg/csv 等数据格式的上传，多个文件请以 zip 压缩包的形式上传到平台，然后再解压缩。当然你也可以通过“下载”按钮把数据集中的文件下载到本地。

<img src='http://imgbed.momodel.cn/5cc1a291e3067ceb154f0e59.jpg' width=40% height=40%>

### 3. 发布公开数据集

点击右上角的编辑按钮，把数据集项目权限设置为公开( Public )，那么数据集就可以被其他人在项目或模块中使用。

## 在 Notebook 中使用数据集

当你需要将数据集插入当前项目或模块的时候,首先打开 一个 Notebook(.ipynb) 文件，然后在数据集页面点击 Import 按钮，此数据集将以只读模式自动挂载到/datasets/<dataset name>
路径下。如果需要修改，可在 Notebook(.ipynb) 中使用

 `!cp -R ./datasets/<imported_dataset_dir>  ./<your_folder>`

 指令将其复制到其他文件夹后再编辑。对于引入的数据集中的 .zip 文件，可使用

`!7zx ./datasets/<imported_dataset_dir>/<XXX.zip> -d ./<your_folder>`

指令解压缩到其他文件夹后使用。

## 如何导入并使用模块和数据集

你可以在训练营中的[平台功能教程](http://www.momodel.cn:8899/classroom/class?id=5c5696cd1afd9458d456bf54&type=doc)，选择`如何导入并使用模块(module)和数据集(dataset)`，来按照教程中的 `.ipynb` 文件指引边学边做，学习如何在开发你的应用(app)时使用模块(module)和数据集(dataset)。

### 1. 新建一个 Notebook

新建的任何项目的 notebook 都默认有下面这个 cell 代码，这部分代码的功能是引入一些默认的包、 Client 和其他一些控制文件。不同的用户和项目下 Client 中的内容不同。

```python
# You can use other public modules via our Client object with module's identifier
# and parameters.
# For more detailes, please see our online document - https://momodel.github.io/docs/#

import os
import sys

# Define root path
sys.path.append('../')

# Import necessary packages
from modules import json_parser
from modules import Client

# Initialise Client object
client = Client(api_key='d26e0dfc13d6f88ff717b6c5d6c0eff5ebb955fa2dd524f6f8db65ede19c9385',
                project_id='5c773bbf7b284e6c93aa02f5', user_ID='5c42bef47b284e741516faad',
                project_type='app', source_file_path='tutorial.ipynb')

# Make run/train/predict command alias for further use
run = client.run
train = client.train
predict = client.predict

# Make controller alias for further use
controller = client.controller
```

### 2. 引入他人发布公开的数据集

机器学习任务中，数据集是至关重要的组成元素之一，如果对于你想完成的项目，你暂时没有合适的数据集，那么可以尝试搜索他人公开的数据集来使用。

点击左侧的 Dataset 栏。按关键字或者热门标签查询相关的数据集。在结果列表中选择合适的数据集，点击打开数据集的详情页面，进一步查看文件列表和作者的说明文档。

点击 Import 按钮，此数据集将以**只读模式**自动挂载到`
/datasets/< imported_dataset_dir >` 路径下。

如果需要修改，可在 Notebook(.ipynb) 中使用
`!cp -R ./datasets/<imported_dataset_dir>  ./<your_folder>`
指令将其复制到其他文件夹后再编辑。

对于引入的数据集中的 .zip 文件，可使用
`!7zx ./datasets/<imported_dataset_dir>/<XXX.zip> -d ./<your_folder>`
指令解压缩到其他文件夹后使用。

那么我们先引入我们在本教程中需要使用到的数据集，我们搜索并插入 star-face 这个数据集。 然后

```shell
!7zx ./datasets/star-face-luxu1220/star-face.zip
```

来解压这个数据集的压缩文件，我们得到一个新的文件夹名为 `star-face`。

### 3. 调用他人发布公开的模块

在 Mo 的 Notebook 中， 我们可以轻松的在我们自己的代码中， 插入别人已经编写好的模块 (我们也称之为 Module )。  有的模块( Module )有的是模型，可以进行二次训练，有的只是一些工具，不能二次训练。我们先看看如何插入一个不能二次训练的工具类的模块。

如下图所示

1. 首先选择想要插入模块代码的 Cell (在这里, 请选择下方 '请点击此处，插入module' 字样的 Cell)。
2. 打开左侧的 Modules 栏，搜索 `iris`，选择 `iris_classifier` 。
3. 在中间区域打开的 Tab 中，浏览 Modules 详情。
4. 点击`插入代码` 按钮，插入 Module (插入代码时，上方会有一个 `导入中...` 的提示框, 表示正在导入模型到项目中)。
5. 插入完成即可点击跳回 `*.ipynb` 的链接, 回到此 Notebook。
6. 用 Notebook 界面顶上的运行按钮 <img src='http://imgbed.momodel.cn/5cc1a28ae3067ceb154f0e4f.jpg' width='40px'>, 或者 `Shift+Enter`, 即可运行插入的 Module。

<img src='http://imgbed.momodel.cn/5cc1a28ce3067ceb154f0e55.gif' width=100% height=100%>

运行后再运行以下代码

```python
iris_class = ['setosa','versicolor','virginica']
classname = iris_class[result[0]]
classname
```

如果你得到了下面的结果，那么恭喜你，成功用公有的模块进行了一次预测。

```
'setosa'
```

### 4. 二次训练模型

如果你插入的模块可以进行二次训练，那么接下来，我们看看如何进行二次训练。

1. 选择想要插入模块代码的 Cell。

2. 前面的教程里我们已经学会了如何使用数据集，并插入了 star-face 这个数据集。如果你还没有的话，请查看第二小节的内容。

3. 然后，打开左侧的 `Modules Tab`，搜索 `new_face_feature` ，选择 `new_face_feature` 。

4. 在中间区域打开的 `Tab` 中，浏览 `Modules` 详情，注意查看各个参数的说明。

5. 在 `Train` 部分点击 `插入代码` 按钮，插入此模块 (插入代码时， 会有一个上方会有一个 `Importing...` 的提示框，表示正在导入模型到项目中)。

6. 插入完成即可点击跳回 `*.ipynb` 的链接，回到此 `Notebook`。

7. 更改训练模块的输入参数，定义

```json
conf={'model_save_path': 'my_model.h5', 'epochs': 2, 'log_dir': './', 'data_path': './star-face', 'weight_save_path': 'my_weight.h5'}
```

8. 然后使用 `shift+enter` 快捷键或者点击上面的运行按钮运行刚才插入的几个cell。
   <img src="http://imgbed.momodel.cn/5cc1a28ae3067ceb154f0e50.jpg" width='100%'>
9. 训练完成后，在左侧可以看到模型和权重文件已经被保存下来了，然后我们就可以在应用中使用它了。

## 开发和部署一个项目

### 1. 简介

应用(app)一般由模块组成, 它能够满足普通用户的直接使用需求，例如航班延误预测应用, 图片风格迁移应用, 或者我们即将部署的兰花分类应用等。
你可以在训练营中的[平台功能教程](http://www.momodel.cn:8899/classroom/class?id=5c5696cd1afd9458d456bf54&type=doc)，选择`开发和部署一个应用(APP)`，来按照教程中的`.ipynb`文件指引边学边做，通过调用别人已经部署的模块(Module)，开发和部署一个应用。

### 2. 新建一个 Notebook

你新建的任何应用Notebook都默认有下面这个cell代码，这部分代码是系统自动生成的，主要作用是引入一些默认的包、Client和其他一些控制文件。
![](http://imgbed.momodel.cn/5cc1a28be3067ceb154f0e53.jpg)

### 3. 调用他人发布公开的模块

在 Mo 的 Notebook 中，我们可以轻松的在我们自己的代码中，插入别人已经编写好的模块 (Module)。
插入过程按以下步骤操作：

1. 首先选择想要插入模块代码的 Cell (在这里, 请选择下方 '请点击此处，插入module' 字样的 Cell)
2. 打开左侧的 Modules Tab， 搜索 `iris`, 选择 `iris_classifier`
3. 在中间区域打开的 Tab 中， 浏览 Modules 详情, 选择对应的版本
4. 点击 `Insert Code` 按钮， 插入 Module (插入代码时, 会有一个上方会有一个 `Importing...` 的提示框， 表示正在导入模型到项目中)
5. 插入完成即可点击跳回 `*.ipynb` 的链接， 回到此 Notebook
6. 用 Notebook 界面顶上的运行按钮 <img src='http://imgbed.momodel.cn/5cc1a28ae3067ceb154f0e4f.jpg' width='30px'>， 或者 `Shift+Enter`， 即可运行插入的 Module

<img src='http://imgbed.momodel.cn/5cc1a28ce3067ceb154f0e55.gif' width=100% height=100%>

如果你得到了下图中的结果， 那么恭喜你， 成功用模块进行了一次预测
<img src='http://imgbed.momodel.cn/5cc1a28ae3067ceb154f0e4d.jpg' width='80%'>

### 4. 二次训练他人发布公开的模型模块

如果引入的模块是可以训练的，那么我们看看如何对引入的模型模块进行二次训练.

1. 选择想要插入模块代码的 Cell 
2. 先引入我们在本教程中需要使用到的数据集，前面的教程里我们已经学会了[如何使用数据集](https://momodel.github.io/docs/#/zh-cn/%E8%BE%B9%E5%AD%A6%E8%BE%B9%E5%81%9A?id=%E5%9C%A8notebook%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86),我们搜索并插入 star-face 这个数据集. 然后` !7zx ./datasets/luxu1220-star-face/star-face.zip `来解压这个数据集的压缩文件,我们得到一个新的文件夹名为 star-face

![](http://imgbed.momodel.cn/5cc1a28be3067ceb154f0e52.jpg)

3. 然后, 打开左侧的 Modules Tab，搜索 `new_face_feature`, 选择 `new_face_feature`

4. 在中间区域打开的 Tab 中， 浏览 Modules 详情， 注意查看各个参数的说明

5. 在 Train 部分点击 插入代码 按钮， 插入此模块 (插入代码时, 会有一个上方会有一个 Importing... 的提示框， 表示正在导入模型到项目中)

6. 插入完成即可点击跳回 *.ipynb 的链接， 回到此 Notebook

7. 更改训练模块的输入参数，定义 `conf={'model_save_path': 'my_model.h5', 'epochs': 2, 'log_dir': './', 'data_path': './star-face', 'weight_save_path': 'my_weight.h5'}`

8. 然后使用 shift+enter 快捷键或者点击上面的运行按钮运行刚才插入的几个cell

![](http://imgbed.momodel.cn/5cc1a28ae3067ceb154f0e50.jpg)

9. 训练完成后，在左侧可以看到模型和权重文件已经被保存下来了，然后我们就可以在预测部分使用它们了.

当然, 我们这里只是一个示例，模型没有很大， 训练的 epoch 也很小，效果提升不明显. 如果你使用的模型模块是基于深度神经网络的，并且网络结构很大，那么训练时间会比较长，这时候可以创建 job 并使用 GPU 来加速训练过程，详见[这里](https://momodel.github.io/docs/#/zh-cn/%E8%BE%B9%E5%AD%A6%E8%BE%B9%E5%81%9A?id=%E5%9C%A8gpucpu%E8%B5%84%E6%BA%90%E4%B8%8A%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B).

### 5. 部署应用

我们的功能代码在 Notebook 调试完成后就可以点击左侧的 Deploy Tab 栏，按照指引部署应用。

1. 开始部署

在 Notebook 中点击左侧栏中的 `Deploy` 按钮，进入部署页面。

<img src='http://imgbed.momodel.cn/5cc1a28be3067ceb154f0e54.jpg' width=40% height=40%>

2. 插入handle函数

handle函数是应用函数的的主函数，也是把输入和输出参数对应起来的接口函数，是部署之后其他人调用你的服务的处理方法。选中 Cell 代码的地方，点击第一步的 Insert “按钮”插入handle函数。然后根据前面调试的功能代码和定义的输入输出参数，整理 handle 函数。请按照该函数中的注释说明规范填写参数结构，这样系统就能自动提取输入输出参数，生成配置文件。

<img src='http://imgbed.momodel.cn/5cc1a28ee3067ceb154f0e56.jpg' width=40% height=40%>

3. 准备部署文件

整理好 handle 函数之后，点击第二步的 start 按钮，开始准备部署时需要的文件。
<img src='http://imgbed.momodel.cn/5cc1a28be3067ceb154f0e51.jpg' width=40% height=40%>

以下为操作过程，首先选择需要部署的代码，handle 函数和新建 Notebook 生成的代码必须选择。然后预览生成的代码，如果有误可以点击上一步重新选择，接下来定义输入输出参数，这里定义四个输入参数，一个输出参数。最后生成 YML 配置文件，系统会根据之前定义的handle函数自动识别参数。通过这个步骤我们生成部署时需要的 Python 脚本和 YML 配置文件。
当然，你可以对生成的 `handler.py` 和 `app_spec.yml` 文件进行进一步的编辑。

![](http://imgbed.momodel.cn/5cc1a28ee3067ceb154f0e57.gif)

4. 部署应用项目

完成所有以上步骤后，点击第3步的 `部署` 按钮进行项目部署。

<img src='http://imgbed.momodel.cn/5cc1a291e3067ceb154f0e58.jpg' width=40% height=40%>

在部署的时候，系统会自动对 `handler.py` 文件和 `app_spec.yml`  配置文件进行基本的格式检查，检查通过后才能进行部署，你可以选择需要发布的文件或勾选发布开发版本或正式版本（开发版本部署后只有所有者可以使用，正式版项目为公开的，所有人可见），然后点击 `OK`，进行部署。

<img src='http://imgbed.momodel.cn/5cc1a28ae3067ceb154f0e4e.jpg' width=100% height=100%>

在短暂的等待后, 回到应用的详情页面, 在右上角会有部署成功的通知。恭喜你！你已经完成了你的第一个APP的部署。

### 6. 运行已部署的应用

然后你就可以找到这个项目，在网页中输入参数，运行得到输出结果，如果发现问题，可再回到项目中进行调试。
<img src='http://imgbed.momodel.cn/5cc1a28ae3067ceb154f0e4c.jpg' width=80% height=80%>

## 在 GPU/CPU 资源上训练机器学习模型

你可以在训练营中的[平台功能教程](http://www.momodel.cn:8899/classroom/class?id=5c5696cd1afd9458d456bf54&type=doc)，选择`在GPU/CPU资源上训练机器学习模型`，来按照教程中的`.ipynb`文件指引边学边做，通过调用别人已经部署的模块(Module)，开发和部署一个应用。

### 1. 简介

在Mo平台上你可以通过两种方式训练你的模型：在Notebook中直接运行、通过建立Job在GPU/CPU后台运行。前者在网页上长时间运行很容易因为各种外部因素而中断，适合短时间小模型的调试训练。后者则通过后台建立Job任务运行，而且可以选择GPU加速，适合长时间大模型的训练。

本次教程采用卷积神经网络对手写数据集 ([MNIST](http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/mnist_beginners.html))进行分类，指引大家学习如何在CPU/GPU资源上训练机器学习模型，本教程采用卷积神经网络进行分类，相较于全连接神经网络具有更好的特征提取能力，能够更好地保存图片的二维特征，在手写数据集上的准确率也会有很大提升。你可以在新建应用项目时选择[从模版创建该教程项目](https://momodel.github.io/docs/#/zh-cn/%E5%B9%B3%E5%8F%B0%E6%95%99%E7%A8%8B?id=%E5%88%9B%E5%BB%BA%E6%95%99%E7%A8%8Bcreate-project)，按照指引完成相应的操作。

### 2. 在 Notebook 中调试训练

首先我们运行下面的 Cell 代码进行必要模块的导入, 以及参数的定义, 我们在这里将每个 epoch 训练的的 batch_size 定为 128

```Python
# -*- coding: utf-8 -*-

'''Trains a simple convnet on the MNIST dataset.

Gets to 99.25% test accuracy after 12 epochs
(there is still a lot of margin for parameter tuning).
16 seconds per epoch on a GPU.
'''

from __future__ import print_function
import numpy as np
np.random.seed(1337)  # for reproducibility

from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
from keras.utils import np_utils
# Keras的底层库使用Theano或TensorFlow
from keras import backend as K

batch_size = 128
nb_classes = 10
nb_epoch = 12

# input image dimensions
img_rows, img_cols = 28, 28
# number of convolutional filters to use
nb_filters = 32
# size of pooling area for max pooling
pool_size = (2, 2)
# convolution kernel size
kernel_size = (3, 3)
```

然后我们导入数据集以及做一些数据预处理

```Python
path = 'keras_mnist_data/mnist.npz'
f = np.load(path)
X_train, y_train = f['x_train'], f['y_train']
X_test, y_test = f['x_test'], f['y_test']
f.close()

if K.image_data_format() == 'channels_first':
    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)
    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)
```

然后使用Keras的Sequential定义两层卷积网络模型

```Python
model = Sequential()

# 卷积层
# 二维卷积层对二维输入进行滑动窗卷积
# keras.layers.convolutional.Convolution2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation='linear', weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='th', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)

# nb_filter：卷积核的数目,（即输出的维度）
# nb_row：卷积核的行数
# nb_col：卷积核的列数
# border_mode：边界模式，为“valid”，“same”或“full”，full需要以theano为后端
model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],
                        border_mode='valid',
                        input_shape=input_shape))
model.add(Activation('relu'))
model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))
model.add(Activation('relu'))

# keras.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th')
# 空域信号施加最大值池化
model.add(MaxPooling2D(pool_size=pool_size))
model.add(Dropout(0.25))

# Flatten层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten不影响batch的大小。
model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])
```

接下来运行下面的 Cell 代码进行训练

```Python
model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
          verbose=1, validation_data=(X_test, Y_test))
score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])
```

*如果觉得训练时间太长, 可以直接点击 Notebook 顶部的<img src='http://imgbed.momodel.cn/5cc1a287e3067ceb154f0e42.jpg' width='30px'>按钮停止程序的运行, 然后到下一小节, 把以上代码转换为py类型的文件，通过创建 Job 任务的方式训练模型。*

最后保存训练好的模型

```Python
model.save('results/my_model.h5')
```

*PS: 这里有个重要的地方, 我们需要将模型保存到 `results/` 文件夹下, 因为这个文件夹是 Job 与 Notebook 的共享文件夹, Job中的训练结果只有保存到 `results/` 下才能被Notebook读取到。*

### 3. 导出代码为 Python 文件

由于加入了深层卷积网络, 此次训练过程可能会比较长, 我们不推荐在 Notebook 中进行长时间训练, 最好的方法是通过创建一个GPU Job后台训练模型。 
Notebook 中的代码是在 *.ipynb 文件下的，为之后创建Job和部署做准备，点击 <img src='http://imgbed.momodel.cn/5cc1a287e3067ceb154f0e43.jpg' width=3% height=3%> 将其转为 `.py` 格式的标准 python 代码。然后整理你的代码，完成测试后，即可进行下一步的操作。  

*如果你是从模版中创建的项目，我们已经为你准备好了一份整理好的 `How_Train_Model.py` 文件, 你可以从左侧 'Files' 文件目录中双击查看, 并直接进行下一步。*  

### 4. 创建 GPU Job 训练模型

点击 Python 编辑器上方的 Create Job, 选择 `GPU Powered Machines`创建 Job，我们可以选择为 Job 输入一个容易辨识名字，当然也可以选择不输入，系统会默认生成。你也可以创建 `Notebook Console` 或 `CPU Only Machines` 形式的 Job ，这需要根据你训练的模型特点选择。

<img src='http://mo-imgs.momodel.cn/jobTutorial.gif' width=80% height=50%>

### 5. 查看 Job 运行进程

<img src='http://imgbed.momodel.cn/5cc1a288e3067ceb154f0e45.jpg' width=50% height=50%>
<img src='http://imgbed.momodel.cn/5cc1a287e3067ceb154f0e44.jpg' width=50% height=50%>

### 6. 训练指标的可视化

 我们会解析你的 job 的输出行并自动将它们转换为训练指标,并进行可视化。

 你需要按照如下格式,一行一行的输出你的训练指标.

```python
print('{"metric": "<choose_metric_name>", "value": <int_or_float>}')
```

 举例来说,你可以这样来记录 accuracy 的变化

```python
 print('{"metric": "accuracy", "value": 0.985}')
 # {"metric": "accuracy", "value": 0.95}
```

或者这样来记录 loss 的变化

```python
print('{{"metric": "loss", "value": {}}}'.format(loss))
# {"metric": "loss", "value": 2.2233}
```

实际上你的 value 值可以是任意的格式,但是我们目前的可视化只支持整型和浮点型数字。

我们默认使用输出的时间来作为 X-axis, 但是你也可以使用 step
或者 epoch 关键字.注意 step 和 epoch 必须是整型数值。

```python
print('{"metric": "<metric_name>", "value": <int_or_float>, "step": <int>}')
```

```python
print('{"metric": "<metric_name>", "value": <int_or_float>, "epoch": <int>}')
```

然后,可以在项目详情页的任务部分,查看可视化结果。
![](http://imgbed.momodel.cn/5cc1a288e3067ceb154f0e46.jpg)

## 利用 TensorBoard 可视化评估模型

你可以在训练营中的[平台功能教程](http://www.momodel.cn:8899/classroom/class?id=5c5696cd1afd9458d456bf54&type=doc)，选择`利用TensorBoard可视化评估模型`，来按照教程中的`.ipynb`文件指引边学边做，学习如何利用TensorBoard可视化评估模型。

### 1. 简介

我们集成了名为 TensorBoard 的可视化工具，来帮助你理解、调试和优化你的TensorFlow机器学习模型。
你可以使用TensorBoard来可视化你的机器学习模型的推理图（graph）结构，显示训练过程中准确率和损失函数的变化，绘制关于图形执行的量化指标等等。下面以手写数据集MNIST为例，来帮助你在Mo平台上更便捷的使用 TensorBoard 可视化工具。更多关于TensorBoard 的信息请参考[官方文档](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) 和 [官方Github开源项目](https://github.com/tensorflow/tensorboard)。你可以在新建应用项目时选择[从模版创建该教程项目](https://momodel.github.io/docs/#/zh-cn/%E5%B9%B3%E5%8F%B0%E6%95%99%E7%A8%8B?id=%E5%88%9B%E5%BB%BA%E6%95%99%E7%A8%8Bcreate-project)，按照指引进行相应的操作。

### 2. 撰写模型代码

导入手写数据集，定义神经网络图结构和训练过程，并在程序代码中添加 TensorBoard 总结指令，具体可以参考 TensorBoard [官方文档](https://www.tensorflow.org/guide/summaries_and_tensorboard?hl=zh-cn)。这里需要注意产生的logs文件要保存在 `./results/tb_results`文件夹。

```Python
import tensorflow as tf

# Import MNIST data
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets("./data", one_hot=True)
print(mnist)
# Parameters
learning_rate = 0.01
training_epochs = 20
batch_size = 100
display_epoch = 1
# ！Important！
# *您需要将logs文件存储到`/results/tb_results`文件夹下*
logs_path = './results/tb_results/tutorial/'

# tf Graph Input
# mnist data image of shape 28*28=784
x = tf.placeholder(tf.float32, [None, 784], name='InputData')
# 0-9 digits recognition => 10 classes
y = tf.placeholder(tf.float32, [None, 10], name='LabelData')

# Set model weights
W = tf.Variable(tf.zeros([784, 10]), name='Weights')
b = tf.Variable(tf.zeros([10]), name='Bias')

# Construct model and encapsulating all ops into scopes, making
# Tensorboard's Graph visualization more convenient
with tf.name_scope('Model'):
    # Model
    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax
with tf.name_scope('Loss'):
    # Minimize error using cross entropy
    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))
with tf.name_scope('SGD'):
    # Gradient Descent
    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
with tf.name_scope('Accuracy'):
    # Accuracy
    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
    acc = tf.reduce_mean(tf.cast(acc, tf.float32))

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()

# Create a summary to monitor cost tensor
tf.summary.scalar("loss", cost)
# Create a summary to monitor accuracy tensor
tf.summary.scalar("accuracy", acc)
# Merge all summaries into a single op
merged_summary_op = tf.summary.merge_all()

# Start training
with tf.Session() as sess:

    # Run the initializer
    sess.run(init)

    # op to write logs to Tensorboard
    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())

    # Training cycle
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = int(mnist.train.num_examples/batch_size)
        # Loop over all batches
        for i in range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            # Run optimization op (backprop), cost op (to get loss value)
            # and summary nodes
            _, c, summary = sess.run([optimizer, cost, merged_summary_op],
                                     feed_dict={x: batch_xs, y: batch_ys})
            # Write logs at every iteration
            summary_writer.add_summary(summary, epoch * total_batch + i)
            # Compute average loss
            avg_cost += c / total_batch
        # Display logs per epoch step
        if (epoch+1) % display_epoch == 0:
            print("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(avg_cost))

    print("Optimization Finished!")

    # Test model
    # Calculate accuracy
    print("Accuracy:", acc.eval({x: mnist.test.images, y: mnist.test.labels}))
```

### 3. 利用 TensorBoard 查看模型结构和训练情况

点击左侧 Running Tab栏，点击TensorBoard图标按钮!<img src='http://imgbed.momodel.cn/5cc1a289e3067ceb154f0e49.jpg' width=10% height=10%>打开TensorBoard网页界面。

<img src='http://imgbed.momodel.cn/5cc1a289e3067ceb154f0e48.jpg' width=50% height=50%>

查看你的模型Graph和训练情况。

<img src='http://imgbed.momodel.cn/5cc1a288e3067ceb154f0e47.gif' width=90% height=90%>

## 把模型转换为 TensorFlow Lite 格式

你可以在训练营中的[平台功能教程](http://www.momodel.cn:8899/classroom/class?id=5c5696cd1afd9458d456bf54&type=doc)，选择`把模型转换为TensorFlow Lite 格式`，来按照教程中的`.ipynb`文件指引边学边做，学习如何利用TensorBoard可视化评估模型。

### 1. 简介

TensorFlow Lite 是 TensorFlow 针对移动和嵌入式设备的轻量级解决方案。它赋予了这些设备在终端本地运行机器学习模型的能力，从而不再需要向云端服务器发送数据。这样一来，不但节省了网络流量、减少了时间开销，而且还充分帮助用户保护自己的隐私和敏感信息。本部分将指引你把训练好的机器学习模型转换为手机或嵌入式设备可以使用的TensorFlow Lite 格式。

### 2. 准备训练好的模型

当转换为TensorFlow Lite格式时，需要将模型和权重都保存在同一个文件中。如果你在 Tensorflow 框架下构建并训练你的模型，你需要将模型保存为SavedModel格式。SavedModel 是一种独立于语言且可恢复的序列化格式，使较高级别的系统和工具可以创建、使用和转换 TensorFlow 模型。创建 SavedModel 的最简单方法是使用 ```tf.saved_model.simple_save``` 函数。更多关于SavedModel 的信息请见[官方文档](https://www.tensorflow.org/programmers_guide/saved_model)。

如果你在 Tensorflow Keras 框架下构建并训练你的模型，你可以使用 ```tf.keras.models.save_model``` 函数将模型和权重都保存在同一个模型文件中，这里我们通过构建一个keras简单模型并且保存训练好的模型，得到```.h5```后缀的模型文件。

```Python
import numpy as np
import tensorflow as tf

# Generate tf.keras model.
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(2, input_shape=(3,)))
model.add(tf.keras.layers.RepeatVector(3))
model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3)))
model.compile(loss=tf.keras.losses.MSE,
              optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),
              metrics=[tf.keras.metrics.categorical_accuracy],
              sample_weight_mode='temporal')

x = np.random.random((1, 3))
y = np.random.random((1, 3, 3))
model.train_on_batch(x, y)
model.predict(x)
```

<img src='http://imgbed.momodel.cn/5cc1a28ae3067ceb154f0e4b.jpg' width=90% height=90%>

然后通过以下代码保存模型，会在当前文件目录中生成 'keras_model.h5' 文件。

```Python
# Save tf.keras model in HDF5 format.
keras_file = "keras_model.h5"
tf.keras.models.save_model(model, keras_file)
```

### 3. 将模型转换为 TensorFlow Lite 格式

选择刚刚保存的`.h5`后缀的模型文件，然后右键选择 ```Convert To TensorLite``` 就可以得到转换之后的 TensorFlow Lite 模型文件。

<img src='http://imgbed.momodel.cn/5cc1a289e3067ceb154f0e4a.jpg' width=50% height=50%>

### 4. 下载转换后的模型并将其嵌入你的本地程序

模型转换完成后，在左侧的文件列表中便可以看到以转换后的以 ```.tflite``` 为后缀的文件。
右键此文件，并点击下载，即可将其下载到你的本地电脑中。然后你可以将其嵌入到你的 Android app、iOS app 或 Raspberry Pi中。更多信息请参阅[TensorFlow官方文档](https://www.tensorflow.org/lite/devguide)

## 如何在 Notebook 中节省运行内存

### 1. 通过指定合适的数据类型减少内存

当我们在读取数据文件时，例如某一列的数据范围肯定在0~255之中，那么我们可以指定为np.uint8类型，如果不手动指定的话默认为np.int64类型，这之间的差距巨大。对于浮点数如果不指定数据类型，默认的精度是float64，但是通常情况下float32，甚至float16的精度就已经足够了。所以在读取数据时指定数据类型可以节省大量内存空间。
例如我们用 pandas 读取一个 csv 文件，分别对比两者的内存占用。

当不指定数据类型时

```python
import pandas as pd
data_1 = pd.read_csv('btc.csv')

# 查看数据类型及内存占用情况
data_1.info()
```

输出信息如下：

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3405857 entries, 0 to 3405856
Data columns (total 8 columns):
Timestamp            int64
Open                 float64
High                 float64
Low                  float64
Close                float64
Volume_(BTC)         float64
Volume_(Currency)    float64
Weighted_Price       float64
dtypes: float64(7), int64(1)
memory usage: 207.9 MB
```

当指定数据类型时

```python
# 指定数据类型
data_2 = pd.read_csv('btc.csv', dtype={'Open':np.float16, 'High':np.float16, 'Low':np.float16, 
                                       'Close':np.float16, 'Volume_(BTC)':np.float16, 
                                       'Volume_(Currency)':np.float16, 'Weighted_Price':np.float16})
data_2.info()
```

输出信息如下：

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3405857 entries, 0 to 3405856
Data columns (total 8 columns):
Timestamp            int64
Open                 float16
High                 float16
Low                  float16
Close                float16
Volume_(BTC)         float16
Volume_(Currency)    float16
Weighted_Price       float16
dtypes: float16(7), int64(1)
memory usage: 90.9 MB
```

从以上对比可以看出，在读取数据时，只通过指定合适的数据类型，就可以减少一倍多的内存空间，是一种简单有效的方法。另外在数据处理的过程中也要及时对数据进行类型转换，减少内存。例如字符串占的内存比数字要大很多，可用数字符号代替字符串。比如我有一个叫 "country" 的列，它的值有'China'， 'USA'， 'Italy'，那么我可以用一个 map 把这三个值分别映射到1，2，3。这样也可以节省内存。

### 2. 通过函数封装减少中间变量内存占用

Python 程序 在 Linux 或者 Mac 中，哪怕是 del 这个对象，Python 依旧不把内存还给系统，自己先占着直到进程销毁。正是由于这样的垃圾回收机制，所以会导致我们在数据处理的过程中产生大量的中间变量，浪费内存资源。一种方式是我们在数据处理过程中尽量减少赋值导致的 COPY, 修改时带上 inplace=True。如果某个中间变量一定要产生，并且我们使用后还需要释放的可以把多步数据处理过程封装到一个函数中，利用函数的内存释放机制避免产生中间变量。特别在使用 Jupyter Notebook 进行数据分析时，避免一步一步的处理方式，多采用函数封装的形式，甚至导入模块的步骤也可以封装到函数中。

以下示例对比在 Jupyter Notebook 中单步执行以下命令和封装成一个函数的内存占用情况：

单步执行时

```python
import psutil
import os
import pandas as pd

# 求解每列的最大最小
df = pd.read_csv('btc.csv')
min_df = df.min()
max_df = df.max()

# 查看内存占用情况
info = psutil.virtual_memory()
print(psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024, 'MB')
```

输出当前使用的内存为 455.03515625 MB

把求解最大最小过程封装成一个函数时

```python
import psutil
import os

# 封装函数
def get_min_max():
    import pandas as pd
    df = pd.read_csv('btc.csv')
    min_df = df.min()
    max_df = df.max()
    return min_df, max_df

# 求解每列的最大最小
min_df, max_df = get_min_max()

# 查看内存占用情况
info = psutil.virtual_memory()
print(psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024, 'MB')
```

输出当前使用的内存为 76.86328125 MB

以上过程中间变量 df 并不是我们所需要的，但在数据处理过程中会占用大量内存，由于 Python 的内存回收机制，在单步执行时即使用 del 命令删除某个中间变量，内存资源也无法释放，所以采用封装函数的方式是一种较好的方式。

### 3. 使用迭代器或生成器读取数据

采用迭代器或生成器的方式读取数据时，并没有真正读取数据，而是在调用 next() ，get_chunk()等函数时才会获取加载真正的数据。这样我们就可以分块读取处理数据，减少一次性加载数据时内存占用过大的问题。

以下示例读取大型 csv 文件时，可以采用迭代器的方式加载，加载数据时基本不占用内存，只有在调用 get_chunk 函数时才会加载指定数量数据。

```python
import pandas as pd
reader = pd.read_csv('btc.csv', iterator=True)

# 通过 get_chunk 函数获取指定数量的数据
df = reader.get_chunk(1000)
```

### 4. 其他减少内存占用的技巧

* 图片数据集读取时只存储图片名称的列表，在需要处理时再加载数据。
* 很多机器学习框架提供了控制内存占用的接口参数，例如，在 TensorFlow 中训练神经网络模型时可以控制 batch_size 的值，从而限制训练过程中的内存占用；在 Keras 中提供 fit_generator 函数，实现逐批次生成数据，按批次训练模型。

### 总结

内存的减少是靠牺牲时间资源或 CPU 资源来换取的，实际使用时要综合权衡。在进行数据分析处理时良好的数据处理思维可以大幅提高我们分析数据的效率，减少不必要的资源浪费。
